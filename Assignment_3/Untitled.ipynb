{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TDT4173: Machine Learning and Case-Based Reasoning\n",
    "\n",
    "## Assignment 4 - Even L. Wanvik - 07.10.2019\n",
    "\n",
    "### 1 Theory [1.5 points]\n",
    "\n",
    "#### 1.1 [0.2 points] What characterises case-based reasoning (CBR) methods? How are they different from other machine learning approaches?\n",
    "\n",
    "The main characteristic of a case-based reasoning method is that it stores previous experiences rather than creating distinct logical rules or neural/bayesian networks. Instead of inductive reasoning, it finds a solution based on transductive reasoning, as it maps the retrieved a previously observed case, supposedly the most similar to our current case, to fit our current target/case.\n",
    "\n",
    "Compared to other rule-inducing algorithms that generalize \"prematurely\" from the training data, a CBR method performs a *lazy generalization*, where it delays the generalization of its cases until testing time. CBR method tends to perform better for more comprehensive and complex domains in which it is effectively impossible to generalize every outcome.\n",
    "\n",
    "#### 1.2 [0.4 points] Discuss (some of) the ways in which cognitive science has influenced CBR.\n",
    "\n",
    "'\n",
    "\n",
    "CBR is related to research within analogical reasoning, which is an area of research in cognitive science. CBR utilizes the cognitive science of how humans derive their actions or conclusions is from finding analogies the most similar previous experiences stored as episodic memory. Like humans learn by remembering new experiences for future use, CBR stores new cases. \n",
    "\n",
    "#### 1.3 [0.3 points] Methods to evaluate the degree of similarity between two cases are essential in CBR. What is the difference between surface similarity and structural similarity? Give some examples for each approach.\n",
    "\n",
    "<img style=\"float: left;\" src=\"images/KnnClassification.png\" style=\"width:50px;\"> \n",
    "Surface similarity is a more high-level approach, where attribute-value vectors are compared both locally (attribute-wise) and globally (weighted average of local similarities). The retrieved case(s) are the k-most similar to our target case/problem in a \"k-nearest neighbor\" fashion, also referred to as k-NN. The image to the left illustrates a simple example of k-NN classification; the green dot is our target case, and the red triangles and blue squares are stored classes of cases. If k is a threshold up until the solid line, our case is classified as a red triangle. If up until the dashed line, our target case is most similar to the square case, etc. Another approach for surface similarity is using a k-d tree, in which the retrieval time is reduced by containing cases similar to each other in groups according to a similarity measure. \n",
    "\n",
    "A structural similarity measure is a more comprehensive method which evaluates the domain and structure of the cases, rather than having a simple attribute-value vector attached to each case. The best example of structural similarity would be in object-oriented programming, where cases are represented by objects. Objects belong to classes and can be classes themselves, hence the complexity. Objects that are close to each other in the \"class hierarchy\" are most likely more similar than objects farther apart. A surface similarity search will at most have an O(n) complexity, n being the number of cases. A structural similarity algorithm, on the other hand, has to measure the similarities between objects in class hierarchies and between the classes themselves, which is much more computationally expensive. One proposed method to alleviate the computational cost is to combine surface and structural similarity in a two-stage retrieval process. Other than the object-oriented approach, we have spreading activation methods, in which an interconnected network of nodes captures \"activation\" (weights) spread from the target, and the most similar response is caught.\n",
    "\n",
    "#### 1.4 [0.3 points] Explain how the similarity between cases can be measured when cases are made up of attributes with different data types. Give an example of how this can be done.\n",
    "\n",
    "* We should be able to use the *euclidean distance* provided that:\n",
    "    * the data can be approximated by a normal distribution, we can re-scale it into a real value between 0 (mean) and 1 (standard deviation). \n",
    "    * We can re-code the textual/categorical elements, e.g. \"male\"/\"female\". \n",
    "    \n",
    "SPØR OM FLERE!\n",
    "\n",
    "#### 1.5  [0.3 points] What are knowledge containers in the context of CBR? Give a brief explanation of the different containers.\n",
    "\n",
    "There are four so-called knowledge containers that are in play when using a CBR approach, namely *vocabulary*, *case base*, *similarity*, and *adaptation*. The initial CBR could just be a large collection of known cases. As the system evolves, knowledge can be fransferred from *case base* to similarity or adaptation containers and maybe even a revised vocabulary. The interplay between the containers is an important part of the systems ability to adapt and evolve.\n",
    "\n",
    "The case-base container holds all pervious experiences as a problem-solution pair. Similarity container is somewhat of a guide of to which case that is most similar to the current problem. Vocabulary specifies the structure and language used to represent, gather and organize the cases. Adaptation modifies the retrieved case solution to fit our target problem.\n",
    "\n",
    "\n",
    "\n",
    "### 2 Practical [1.5 points]\n",
    "\n",
    "\n",
    "#### 2.1 Case Modelling [0.5 points]\n",
    "\n",
    "##### a) Create a new concept called patient, which will be used as the basis for the rest of this assignment.\n",
    "##### b) Create 4-6 relevant attributes for the patient concept, including name, weight, and sleep quality.\n",
    "I chose to use:\n",
    "* name – a purely descriptive string.\n",
    "* weight_m – a float value representing the weight of the patient in kilograms.\n",
    "* sleep_quality – a symbol type with at least low, medium, and high as allowed.\n",
    "* sex - a symbol with either F (female) or male (male)\n",
    "* height_m - a float value representing the heigh of the patient in meters.\n",
    "* age - a integer value representing the age of the patient.\n",
    "\n",
    "##### c) Create ten or more instances of the patient concept, for each instance you must fill out all of the attributes.\n",
    "\n",
    "##### d) Include a screenshot of 2-3 of the instances in your report.\n",
    "\n",
    "\n",
    "<img style=\"float: left;\" src=\"images/patient0.png\" style=\"width:50px;\">\n",
    "<img style=\"float: left;\" src=\"images/patient1.png\" style=\"width:50px;\"> \n",
    "<img style=\"float: left;\" src=\"images/patient2.png\" style=\"width:50px;\"> \n",
    "\n",
    "\n",
    "\n",
    "#### 2.2 Case Retrieval [1 point]\n",
    "\n",
    "##### e) [0.3 points] Create a global similarity measure for the patient concept. It should ignore the name attribute. Create a few different similarity measures for the weight attribute, including one where the similarity is 1 for an exact match, and otherwise decreasing linearly towards 0 the further away the weight values are apart. Select the other attribute similarity modes and comparison functions as you see fit\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
