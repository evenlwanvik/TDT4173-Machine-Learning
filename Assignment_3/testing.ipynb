{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Tested a lot of algorithms.. Just want to store them, just in case i need inspo for later algorithms xD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "train = pd.read_csv(\"./dataset/adaboost_train.csv\")\n",
    "train_X = train[list(train)[2:]]\n",
    "train_Y = train[list(train)[1]]\n",
    "test  = pd.read_csv(\"./dataset/adaboost_test.csv\")\n",
    "test_X = test[list(test)[2:]]\n",
    "test_Y = test[list(test)[1]]\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, X_train, Y_train, T, X_test, Y_test):\n",
    "        self.X_train = X_train\n",
    "        self.Y_train = Y_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = X_test\n",
    "        self.T = T\n",
    "        self.alphas = None\n",
    "        self.models = None\n",
    "        self.accuracy = []\n",
    "        self.predictions = None\n",
    "\n",
    "    def train(self):\n",
    "        X = self.X_train\n",
    "        Y = self.Y_train\n",
    "        y_eval = pd.DataFrame(data=Y.copy())\n",
    "        y_eval['weights'] = 1/len(Y) # Append a weights column for evaluation\n",
    "\n",
    "        models = list()\n",
    "        alphas = list()\n",
    "        \n",
    "        for t in range(self.T):\n",
    "\n",
    "            stump = DecisionTreeClassifier(max_depth=1)\n",
    "            # Since our decision tree is a stump, the root will give us the most general decision\n",
    "            model = stump.fit(X, Y, sample_weight=np.array(y_eval['weights']))\n",
    "            models.append(model)\n",
    "            # Compare the predicted classifications with actual classification\n",
    "            y_eval['predictions']   = model.predict(X)\n",
    "            y_eval['evaluation']    = np.where(y_eval['predictions']==Y, 1, 0)\n",
    "            y_eval['misclassified'] = np.where(y_eval['predictions']!=Y, 1, 0)\n",
    "            accuracy = np.sum( y_eval['evaluation'] / len(y_eval['evaluation']) )\n",
    "            misclassified = np.sum( y_eval['misclassified'] / len(y_eval['misclassified']) )\n",
    "            \n",
    "            # Calculate the error rate\n",
    "            error = np.sum( np.dot(y_eval['misclassified'],y_eval['weights']) / np.sum(y_eval['weights']) )\n",
    "            # Calculate alpha (weight for classifier)\n",
    "            alpha = 0.5*np.log( (1-error) / error )\n",
    "            alphas.append(alpha)\n",
    "            \n",
    "            # Update the weights for training next stump\n",
    "            y_eval['weights'] = np.where(y_eval['misclassified']==1, \n",
    "                                         y_eval['weights']*np.exp(alpha),\n",
    "                                         y_eval['weights']*np.exp(-alpha))\n",
    "            # Rounding errors cause sum of weights to either be lower or higher than 1, normalize it\n",
    "            y_eval['weights'] = y_eval['weights']/np.sum(y_eval['weights'])\n",
    "            \n",
    "            #y_eval['weights'] *= np.exp(alpha*y_eval['misclassified'])\n",
    "            \n",
    "            #print('The Accuracy of the {0}. model is : '.format(t+1),accuracy*100,'%')\n",
    "            #print('The missclassification rate is: ',misclassified*100,'%')\n",
    "           \n",
    "        self.alphas = alphas\n",
    "        self.models = models\n",
    "\n",
    "    def predict(self):\n",
    "        X_test = self.X_test \n",
    "        Y_test = self.Y_test \n",
    "        \n",
    "        accuracy = list()\n",
    "        predictions = list()\n",
    "        \n",
    "        # iterates over (alpha[i], model[i])\n",
    "        for alpha,model in zip(self.alphas, self.models):\n",
    "            # Append weak-classifier (stump) prediction\n",
    "            predictions.append( alpha*model.predict(X_test) )\n",
    "            \n",
    "            # This gives deprecation error\n",
    "            self.accuracy.append(np.sum(np.sign(np.sum(np.array(predictions),axis=0))==Y_test.values)/len(predictions[0]))\n",
    "            \n",
    "        self.predictions = np.sign(np.sum(np.array(predictions),axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create T stumps\n",
    "for stump in range(T):\n",
    "    features = list(train)[2:-1]\n",
    "    err = np.zeros(len(features))\n",
    "    for i,f in enumerate(train[features]):\n",
    "        X = train[f]\n",
    "        # Use mean value of feature to decide -1 or 1\n",
    "        Xavg = np.mean(X)\n",
    "        # Sum weighted classification errors of each sample\n",
    "        for j in range(N_train):\n",
    "            if (X[j]>Xavg):\n",
    "                # If X is 1 and row is not 1 we have a classification error\n",
    "                if not train['y'][j] == 1:\n",
    "                    err[i] = err[i] + W[j]\n",
    "                    \n",
    "            else:\n",
    "                # And vice versa\n",
    "                if not train['y'][j] == -1:\n",
    "                    err[i] = err[i] + W[j]\n",
    "    # Choose the feature which got the most classification error\n",
    "    idx, errval = max(enumerate(err), key=lambda e: e[1])\n",
    "    print(f\"x{idx} has largest error: {errval}\")\n",
    "    a = np.log((1-errval)/errval)\n",
    "    # Update all the weights according to the error found in this binary classifier\n",
    "    target = train[f'x{idx}']\n",
    "    Xavg = np.mean(target)\n",
    "    \n",
    "    test = DecisionTreeClassifier(max_depth=1)\n",
    "    \n",
    "    for i,sample in enumerate(target):\n",
    "        # Same as before, only now we update the weights of each sample (s)\n",
    "        if (sample>Xavg):\n",
    "            if not train['y'][i] == 1:\n",
    "                W[i] = W[i]*np.exp(errval)\n",
    "            else:\n",
    "                W[i] = W[i]*np.exp(-errval)\n",
    "        else:\n",
    "            if not train['y'][i] == -1:\n",
    "                W[i] = W[i]*np.exp(errval)\n",
    "            else:\n",
    "                W[i] = W[i]*np.exp(-errval)\n",
    "    # Normalize weight\n",
    "    W = W/sum(W)\n",
    "    \n",
    "# Calculate gini index for each stump"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
